[
  {
    "category": "infrastructure",
    "description": "Scaffold Next.js 14 app with TypeScript, ESLint, Prettier, and folder structure",
    "steps": [
      "Run: npx create-next-app@latest . --typescript --eslint --app --src-dir --import-alias '@/*' --no-tailwind",
      "Install prettier and configure .prettierrc",
      "Create src/features/, src/shared/, src/app/ directories following FSD",
      "Verify: npm run build passes"
    ],
    "passes": true
  },
  {
    "category": "infrastructure",
    "description": "Install and configure core dependencies: AI SDK (RouteLLM/Abacus.ai), MCP client, ElevenLabs",
    "steps": [
      "npm install ai @ai-sdk/openai @modelcontextprotocol/sdk",
      "RouteLLM uses OpenAI-compatible API: set ABACUS_API_KEY + ABACUS_BASE_URL=https://api.abacus.ai/api/v0/routellm in .env.local",
      "Add ELEVENLABS_API_KEY, MCP_ZAPIER_URL to .env.local and .env.example",
      "Create src/shared/lib/llm.ts — createOpenAI() with custom baseURL pointing to Abacus RouteLLM endpoint",
      "Verify: npm run dev starts without errors"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Build core chat UI: message list, input bar, send button",
    "steps": [
      "Create src/features/chat/components/ChatWindow.tsx — scrollable message list",
      "Create src/features/chat/components/MessageBubble.tsx — user vs assistant styling",
      "Create src/features/chat/components/ChatInput.tsx — textarea + send button",
      "Wire together in src/app/page.tsx",
      "Verify: user can type and see echoed messages in UI"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Create /api/chat route with RouteLLM streaming via Vercel AI SDK",
    "steps": [
      "Create src/app/api/chat/route.ts",
      "Import routeLLM from src/shared/lib/llm.ts",
      "Use streamText() + convertToModelMessages() with model='route-llm'",
      "Return result.toUIMessageStreamResponse()"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Integrate MCP client for Zapier (Google Drive / Sheets tools)",
    "steps": [
      "Read MCP SDK docs via context7",
      "Create src/shared/lib/mcp-client.ts — connect to Zapier MCP endpoint (SSE or stdio)",
      "Configure MCP_ZAPIER_URL env var pointing to Zapier MCP server URL",
      "List available tools from MCP server on startup, log them",
      "Expose tools to AI SDK via experimental_prepareToolCall or tools option in streamText()",
      "Test: ask LLM 'list my Google Drive files' and verify tool call executes"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Implement Google Sheets row-append tool via MCP/Zapier",
    "steps": [
      "Identify Zapier MCP tool for appending rows to a Google Sheet",
      "Register tool schema in the tools passed to streamText()",
      "Handle tool result and return it to the LLM for natural-language response",
      "Test in chat: 'Add row [Name, Age] to my spreadsheet' triggers correct Zapier action",
      "Show tool call status in UI (e.g. 'calling tool...' indicator)"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Implement Google Drive file read/update via MCP/Zapier",
    "steps": [
      "Map Zapier MCP tools for reading file content and updating file content",
      "Register schemas and handle results in /api/chat route",
      "Test: 'Read my doc titled X' returns file content in chat",
      "Test: 'Update doc X to say Y' triggers write tool",
      "Add error handling for MCP timeouts and auth failures"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Integrate ElevenLabs TTS: play assistant responses as audio",
    "steps": [
      "Create /api/tts/route.ts — accepts text, calls ElevenLabs REST API, returns audio stream",
      "Use ELEVENLABS_API_KEY from env, pick a default voice ID",
      "Create src/features/chat/hooks/useTTS.ts — fetches audio blob and plays via Web Audio API",
      "Add speaker icon button on each assistant MessageBubble to trigger TTS",
      "Verify: clicking speaker plays audible speech"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Add auto-play TTS mode toggle in chat settings",
    "steps": [
      "Add global state (zustand or React context) for autoPlayTTS: boolean",
      "Create src/features/chat/components/ChatSettings.tsx with toggle switch",
      "In ChatWindow, auto-call useTTS on each new assistant message when flag is true",
      "Persist setting to localStorage",
      "Verify: toggle ON → each reply plays; toggle OFF → silent"
    ],
    "passes": true
  },
  {
    "category": "styling",
    "description": "Design premium dark-mode chat UI with micro-animations",
    "steps": [
      "Create src/app/globals.css with CSS custom properties: colors, spacing, radius, shadows",
      "Dark theme by default: deep navy/charcoal background, accent color for user bubbles",
      "Add smooth scroll-to-bottom animation on new messages",
      "Animate message bubbles fade-in on arrival (CSS keyframes)",
      "Typing indicator component (three bouncing dots) shown while LLM streams",
      "Verify: visually polished, no layout shifts"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Add conversation history: persist chat sessions to localStorage",
    "steps": [
      "Create src/features/chat/lib/storage.ts — save/load message arrays keyed by session ID",
      "Create sidebar component listing past sessions",
      "On session select, load messages into chat state",
      "New chat button creates fresh session",
      "Verify: refresh page, history restored"
    ],
    "passes": true
  },
  {
    "category": "infrastructure",
    "description": "Environment validation, input sanitization, and security hardening",
    "steps": [
      "Install zod; validate all env vars at startup via src/shared/lib/env.ts",
      "Validate and sanitize /api/chat request body with zod schema",
      "Add rate-limiting middleware to /api/chat (simple in-memory limiter or upstash/ratelimit)",
      "Ensure .env.local never committed (.gitignore check)",
      "Run npm audit; fix any high/critical vulnerabilities"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Multiple Models - Choose from several AI models for chat with different capabilities",
    "steps": [
      "Add model selector dropdown to ChatWindow header",
      "Persist selected model in global state/localStorage",
      "Update /api/chat POST to accept model ID",
      "Pass chosen model to RouteLLM/Abacus config"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Prompt & Agent Management - Change agent prompts and switch specialized agents",
    "steps": [
      "Create Settings modal to define custom system prompts",
      "Add preset agents (e.g., Coder, Support, Writer)",
      "UI to switch active agent",
      "Pass selected agent's system prompt to /api/chat"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Support Automation - Automatic ticket routing with support agents coordination",
    "steps": [
      "Detect support intent in user messages via LLM tool/router",
      "Extract ticket metadata (category, urgency)",
      "Route to specialized simulated support agent",
      "Track ticket status and handoffs in UI"
    ],
    "passes": true
  },
  {
    "category": "infrastructure",
    "description": "Multi-channel Support - Chat works on Telegram, Discord, etc.",
    "steps": [
      "Create generic webhook route /api/webhooks/[channel]",
      "Integrate Telegram Bot API (receive/send messages)",
      "Route webhook messages to existing LLM logic",
      "Format and dispatch responses back to channel"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Voice Reply to Text - Advanced TTS for all incoming texts",
    "steps": [
      "Enhance existing TTS to stream chunks for zero-latency playback",
      "Add audio visualizer component during speech",
      "Ensure voice matches active agent persona",
      "Verify robust auto-play without user interaction on supported browsers"
    ],
    "passes": true
  },
  {
    "category": "infrastructure",
    "description": "RAG Phase 1 - Setup LlamaIndex & Vector DB",
    "steps": [
      "Install LlamaIndex TS and Vector DB dependencies (e.g., pgvector/supabase or local in-memory)",
      "Create database tables/schemas for vector embeddings",
      "Initialize VectorStore index client in shared/lib",
      "Verify database connection and index initialization"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "RAG Phase 2 - PDF Parsing and Ingestion",
    "steps": [
      "Implement robust PDF text extraction for large documents (500 pages)",
      "Create chunking logic to split text into reasonable context windows",
      "Generate embeddings for chunks and insert into Vector DB",
      "Create an admin script or endpoint to trigger ingestion"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "RAG Phase 3 - Agent Query Tool",
    "steps": [
      "Create a search tool that queries the Vector DB using user input",
      "Expose tool to the main chat LLM via MCP or AI SDK tools array",
      "Update system prompt to instruct agent to use tool for document queries",
      "Verify LLM successfully retrieves and cites PDF context"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Summarization Phase 1 - Chunking & Map-Reduce Logic",
    "steps": [
      "Implement map-reduce (shotgun) summarization algorithm for large texts",
      "Create individual LLM prompts for chunk summaries and final merged summary",
      "Handle token limits and rate limits during parallel chunk processing"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Summarization Phase 2 - API & UI Integration",
    "steps": [
      "Create /api/summarize endpoint exposing the map-reduce logic",
      "Add UI component for users to upload or select a PDF for summarization",
      "Display streaming or segmented summary results in the chat",
      "Verify end-to-end summarization accuracy on a sample large document"
    ],
    "passes": true
  },
  {
    "category": "infrastructure",
    "description": "Multi-Agent Phase 1 - Orchestration Setup",
    "steps": [
      "Install AutoGen or set up custom multi-agent orchestration architecture",
      "Define personas (e.g., researcher, reviewer, final responder)",
      "Implement the internal message passing loop between agents",
      "Add stop conditions so agents don't loop endlessly"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Multi-Agent Phase 2 - UI Rendering",
    "steps": [
      "Update ChatWindow to handle streaming updates from multiple distinct agents",
      "Add visual indicators for agent thought processes and inter-agent dialogue",
      "Ensure final consensus message is clearly presented to the user"
    ],
    "passes": true
  },
  {
    "category": "functional",
    "description": "Context Priority Phase 1 - Intent Routing",
    "steps": [
      "Create a fast, lightweight LLM routing call for incoming messages",
      "Define intent categories (e.g., casual, technical, document search, form help)",
      "Parse router output to determine the target sub-agent or system prompt"
    ],
    "passes": false
  },
  {
    "category": "functional",
    "description": "Context Priority Phase 2 - Dynamic Swapping",
    "steps": [
      "Update /api/chat to dynamically inject the correct system prompt based on router",
      "Dynamically filter available tools based on the chosen context scope",
      "Verify agent smoothly switches from coding help to casual chat without hallucination"
    ],
    "passes": false
  },
  {
    "category": "functional",
    "description": "Page Context Phase 1 - DOM State Capture",
    "steps": [
      "Create a React hook or utility to read active form fields or page state",
      "Serialize relevant DOM information (e.g., focused input, filled resume fields) into a clean format",
      "Ensure this runs efficiently without causing React render loops"
    ],
    "passes": false
  },
  {
    "category": "functional",
    "description": "Page Context Phase 2 - LLM Context Injection",
    "steps": [
      "Attach serialized page state as metadata in the chat request payload",
      "Append page context to the backend system prompt seamlessly",
      "Test user asking 'What should I put in this field?' and verify accurate response"
    ],
    "passes": false
  },
  {
    "category": "infrastructure",
    "description": "Voice Call Phase 1 - Audio Webhooks",
    "steps": [
      "Set up Telegram/WhatsApp webhook endpoints to accept incoming audio streams or voice notes",
      "Integrate Speech-To-Text (e.g., Whisper API) to transcribe incoming audio",
      "Map transcribed text to the existing text-based LLM chat flow"
    ],
    "passes": false
  },
  {
    "category": "functional",
    "description": "Voice Call Phase 2 - TTS & Responses",
    "steps": [
      "Use existing TTS pipeline to generate audio response from LLM text",
      "Send generated audio file back through the Telegram/WhatsApp API",
      "Handle API rate limits and file size constraints for voice assets"
    ],
    "passes": false
  },
  {
    "category": "functional",
    "description": "Voice Call Phase 3 - Full Contextual Integration",
    "steps": [
      "Ensure the webhook pipeline has access to user chat history and session context",
      "Inject page context (from active web sessions, if linked to the phone number) into the voice LLM prompt",
      "Verify end-to-end voice conversation maintains context across both voice and text channels"
    ],
    "passes": false
  }
]
